<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Research - Jason Trinh</title>
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,600&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Open Sans', sans-serif;
      line-height: 1.6;
      max-width: 800px;
      margin: auto;
      padding: 20px;
      background-color: #ffffff;
    }
    .nav {
      margin-top: -10px;
      margin-bottom: 30px;
    }
    .nav a {
      margin-right: 20px;
      padding: 6px 12px;
      text-decoration: none;
      border-radius: 4px;
      font-weight: 600;
      background-color: #f0f0f0;
      color: #000;
    }
    .nav a.active {
      background-color: #dcdcdc;
    }
    details {
      margin-bottom: 1em;
    }
    summary {
      font-weight: bold;
      cursor: pointer;
    }
    a {
      color: #0645ad;
    }
    code {
      background-color: #f4f4f4;
      padding: 2px 4px;
      border: 1px solid #ccc;
      border-radius: 3px;
      font-family: monospace;
    }
  </style>
</head>
<body>
  <h1>Jason Trinh</h1>
  <div class="nav">
    <a href="index.html">Home</a>
    <a href="projects.html">Projects</a>
    <a href="research.html" class="active">Research</a>
  </div>

  <h1>Research</h1>
  <p>
    Selected research and exploratory work in machine learning, optimization, and scientific computing.
  </p>
  
  <!-- Polar / Muon / Optimizer Experiments -->
  <h2>Optimizer Advancements for Transformer Models</h2>
  <p>
    Currently working on extending the <em>Polar Express</em> variant of the Muon optimizer on GPT-2–style models. The focus is on
    making Muon+Polar-Express fully configurable (polynomial degree, iteration count, safety/cushion factors, and apply frequency),
    and then testing whether attention layers stay stable at higher learning rates compared to plain Muon and AdamW.
  </p>
  <ul>
    <li><strong>Phase 1:</strong> hyperparameter sweep for Polar-Express-in-Muon on small/medium GPT-2 (FineWeb, tens of millions of tokens) to find a fast-but-stable config.</li>
    <li><strong>Phase 2:</strong> attention-layer analysis comparing <strong>AdamW</strong> vs <strong>Muon (legacy)</strong> vs <strong>Muon + Polar Express</strong>, logging attention entropy and QK/√d.</li>
    <li>All runs streamed to <strong>Weights &amp; Biases</strong> (loss, step/PE time, orthogonality error, GPU metrics) for side-by-side comparisons.</li>
    <li>Implemented per-layer orthogonality diagnostics to verify the polar step is actually improving conditioning.</li>
    <li>Runs executed on dual NVIDIA A6000 + BeeGFS storage cluster; triage runs on a 4070/5070 node.</li>
  </ul>

</body>
</html>
